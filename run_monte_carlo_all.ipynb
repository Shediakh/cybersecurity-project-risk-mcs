{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1373325c-3be9-4d30-b6a8-1470cb36cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346d2cb6-5007-43ad-9f84-441a0d942a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Settings\n",
    "# =========================\n",
    "DATA_DIR = \"synthetic_data\"\n",
    "N_ITER = 10_000\n",
    "SEED = 123\n",
    "OUT_DIR = \"mc_outputs\"\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704df1b7-a07c-45b3-890f-a691bdf1d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Graph utilities\n",
    "# =========================\n",
    "def topological_order(task_codes: List[str], preds_map: Dict[str, List[str]]) -> List[str]:\n",
    "    indeg = {t: 0 for t in task_codes}\n",
    "    graph = defaultdict(list)\n",
    "    for t in task_codes:\n",
    "        for p in preds_map.get(t, []):\n",
    "            if p not in indeg:\n",
    "                continue\n",
    "            graph[p].append(t)\n",
    "            indeg[t] += 1\n",
    "    q = deque([t for t in task_codes if indeg[t] == 0])\n",
    "    order = []\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        order.append(u)\n",
    "        for v in graph[u]:\n",
    "            indeg[v] -= 1\n",
    "            if indeg[v] == 0:\n",
    "                q.append(v)\n",
    "    if len(order) != len(task_codes):\n",
    "        raise ValueError(\"Cycle detected or missing nodes in DAG.\")\n",
    "    return order\n",
    "\n",
    "\n",
    "def critical_path_duration(order: List[str], preds_map: Dict[str, List[str]], dur: Dict[str, float]) -> float:\n",
    "    ef = {}\n",
    "    for t in order:\n",
    "        preds = preds_map.get(t, [])\n",
    "        start = max([ef[p] for p in preds], default=0.0)\n",
    "        ef[t] = start + float(dur[t])\n",
    "    return max(ef.values()) if ef else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94e8ff6-6ef7-4881-b2c0-3fe57a0c3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Risk application\n",
    "# =========================\n",
    "def apply_risks_once(\n",
    "    base_dur: Dict[str, float],\n",
    "    risks: pd.DataFrame,\n",
    "    rng: np.random.Generator\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    \"\"\"\n",
    "    Returns adjusted durations and extra lump cost for ONE iteration.\n",
    "    Risk types:\n",
    "      - ADD: add triangular days to target tasks\n",
    "      - MUL: multiply target tasks by lognormal, AND also add triangular days (for late testing drag)\n",
    "    \"\"\"\n",
    "    dur = dict(base_dur)\n",
    "    extra_cost = 0.0\n",
    "\n",
    "    for _, rrow in risks.iterrows():\n",
    "        p = float(rrow[\"probability\"])\n",
    "        if rng.random() >= p:\n",
    "            continue  # risk did not occur\n",
    "\n",
    "        targets = str(rrow[\"targets\"]).split(\";\") if pd.notna(rrow[\"targets\"]) else []\n",
    "        targets = [t for t in targets if t in dur]\n",
    "\n",
    "        # schedule additive piece (always available)\n",
    "        add_o = float(rrow[\"sched_add_tri_o\"])\n",
    "        add_m = float(rrow[\"sched_add_tri_m\"])\n",
    "        add_p = float(rrow[\"sched_add_tri_p\"])\n",
    "        add_days = float(rng.triangular(add_o, add_m, add_p))\n",
    "\n",
    "        # cost lump sum\n",
    "        c_o = float(rrow[\"cost_lump_tri_o\"])\n",
    "        c_m = float(rrow[\"cost_lump_tri_m\"])\n",
    "        c_p = float(rrow[\"cost_lump_tri_p\"])\n",
    "        extra_cost += float(rng.triangular(c_o, c_m, c_p))\n",
    "\n",
    "        if rrow[\"risk_type\"] == \"ADD\":\n",
    "            for t in targets:\n",
    "                dur[t] += add_days\n",
    "\n",
    "        elif rrow[\"risk_type\"] == \"MUL\":\n",
    "            mu = float(rrow[\"mul_logn_mu\"])\n",
    "            sigma = float(rrow[\"mul_logn_sigma\"])\n",
    "            factor = float(rng.lognormal(mu, sigma))\n",
    "            for t in targets:\n",
    "                dur[t] *= factor\n",
    "                dur[t] += add_days  # extra drag on top\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown risk_type: {rrow['risk_type']}\")\n",
    "\n",
    "    return dur, extra_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d9a628-5c7c-4f79-965d-41390a0d2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01: CPM 77.1d | MC P50 79.8d P90 111.1d | P(cost>CPM)=0.78\n",
      "P02: CPM 101.7d | MC P50 110.8d P90 149.3d | P(cost>CPM)=0.87\n",
      "P03: CPM 146.7d | MC P50 171.9d P90 223.7d | P(cost>CPM)=0.94\n",
      "P04: CPM 75.3d | MC P50 111.6d P90 173.3d | P(cost>CPM)=0.97\n",
      "P05: CPM 108.1d | MC P50 115.9d P90 148.8d | P(cost>CPM)=0.85\n",
      "P06: CPM 133.1d | MC P50 143.4d P90 181.6d | P(cost>CPM)=0.84\n",
      "P07: CPM 73.1d | MC P50 93.6d P90 142.3d | P(cost>CPM)=0.94\n",
      "P08: CPM 94.4d | MC P50 131.0d P90 200.2d | P(cost>CPM)=0.97\n",
      "P09: CPM 126.9d | MC P50 131.8d P90 149.1d | P(cost>CPM)=0.76\n",
      "P10: CPM 74.1d | MC P50 83.8d P90 125.3d | P(cost>CPM)=0.88\n",
      "P11: CPM 98.2d | MC P50 114.8d P90 163.7d | P(cost>CPM)=0.93\n",
      "P12: CPM 118.1d | MC P50 156.0d P90 230.5d | P(cost>CPM)=0.97\n",
      "P13: CPM 77.4d | MC P50 81.0d P90 113.4d | P(cost>CPM)=0.81\n",
      "P14: CPM 86.2d | MC P50 93.5d P90 132.1d | P(cost>CPM)=0.81\n",
      "P15: CPM 124.2d | MC P50 147.0d P90 194.9d | P(cost>CPM)=0.95\n",
      "P16: CPM 82.8d | MC P50 117.2d P90 182.5d | P(cost>CPM)=0.95\n",
      "P17: CPM 84.7d | MC P50 88.5d P90 107.4d | P(cost>CPM)=0.86\n",
      "P18: CPM 152.1d | MC P50 161.6d P90 203.1d | P(cost>CPM)=0.85\n",
      "P19: CPM 67.1d | MC P50 84.2d P90 127.9d | P(cost>CPM)=0.95\n",
      "P20: CPM 105.0d | MC P50 140.7d P90 211.9d | P(cost>CPM)=0.96\n",
      "P21: CPM 136.5d | MC P50 140.1d P90 157.4d | P(cost>CPM)=0.69\n",
      "P22: CPM 82.9d | MC P50 90.8d P90 128.4d | P(cost>CPM)=0.87\n",
      "P23: CPM 97.7d | MC P50 115.2d P90 162.5d | P(cost>CPM)=0.95\n",
      "P24: CPM 142.7d | MC P50 178.5d P90 260.3d | P(cost>CPM)=0.95\n",
      "\n",
      "Saved:\n",
      " - mc_outputs\\project_summary.csv\n",
      " - mc_outputs\\sim_long.csv\n",
      "\n",
      "Summary head:\n",
      "   project_id size_bucket risk_level_bucket late_concentration    tail_type  \\\n",
      "11        P12       LARGE           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "23        P24       LARGE           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "7         P08      MEDIUM           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "19        P20      MEDIUM           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "3         P04       SMALL           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "15        P16       SMALL           EXTREME         LATE_HEAVY   HEAVY_TAIL   \n",
      "2         P03       LARGE              HIGH         LATE_HEAVY  NORMAL_TAIL   \n",
      "14        P15       LARGE              HIGH         LATE_HEAVY  NORMAL_TAIL   \n",
      "\n",
      "   coupling  n_tasks  n_streams  burn_rate_per_day  fixed_cost  ...  \\\n",
      "11     WEAK       15          3            3838.76    64279.33  ...   \n",
      "23   STRONG       15          3            3311.26    93626.18  ...   \n",
      "7    STRONG       14          2            2246.73    79697.40  ...   \n",
      "19     WEAK       14          2            5910.50    50628.83  ...   \n",
      "3      WEAK       13          1            4959.53    44529.55  ...   \n",
      "15   STRONG       13          1            3315.71    40460.70  ...   \n",
      "2      WEAK       15          3            5169.38    62182.96  ...   \n",
      "14   STRONG       15          3            3315.71    31321.36  ...   \n",
      "\n",
      "    mc_duration_mean  mc_duration_p50  mc_duration_p80  mc_duration_p90  \\\n",
      "11        167.668465       155.963040       200.565084       230.546563   \n",
      "23        192.693696       178.475022       227.549603       260.331314   \n",
      "7         141.690014       131.027631       174.768686       200.194999   \n",
      "19        152.032445       140.711519       185.620231       211.873950   \n",
      "3         119.637301       111.603499       152.023657       173.260987   \n",
      "15        126.353256       117.151504       158.917499       182.523767   \n",
      "2         180.306131       171.942124       205.137195       223.707570   \n",
      "14        155.030017       147.003759       178.297667       194.915511   \n",
      "\n",
      "    mc_cost_mean    mc_cost_p50   mc_cost_p80   mc_cost_p90  \\\n",
      "11  7.504837e+05  707537.850661  9.003556e+05  1.021008e+06   \n",
      "23  7.742592e+05  730982.954484  9.149437e+05  1.025343e+06   \n",
      "7   4.400190e+05  421137.597954  5.390416e+05  6.011959e+05   \n",
      "19  9.915660e+05  927372.271437  1.215015e+06  1.376727e+06   \n",
      "3   6.804095e+05  646979.466839  8.620303e+05  9.724194e+05   \n",
      "15  5.013200e+05  474928.665267  6.338790e+05  7.187012e+05   \n",
      "2   1.021344e+06  978720.422935  1.164554e+06  1.264045e+06   \n",
      "14  5.725471e+05  548350.855810  6.639478e+05  7.249783e+05   \n",
      "\n",
      "    p_duration_over_cpm  p_cost_over_cpm  \n",
      "11               0.9536           0.9662  \n",
      "23               0.9290           0.9495  \n",
      "7                0.9610           0.9682  \n",
      "19               0.9509           0.9591  \n",
      "3                0.9637           0.9660  \n",
      "15               0.9414           0.9464  \n",
      "2                0.9242           0.9386  \n",
      "14               0.9408           0.9503  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main run\n",
    "# =========================\n",
    "def run():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    projects = pd.read_csv(os.path.join(DATA_DIR, \"projects.csv\"))\n",
    "    tasks = pd.read_csv(os.path.join(DATA_DIR, \"tasks.csv\"))\n",
    "    risks = pd.read_csv(os.path.join(DATA_DIR, \"risks.csv\"))\n",
    "\n",
    "    summary_rows = []\n",
    "    long_rows = []\n",
    "\n",
    "    for pid, pinfo in projects.set_index(\"project_id\").iterrows():\n",
    "        tdf = tasks[tasks[\"project_id\"] == pid].copy()\n",
    "        rdf = risks[risks[\"project_id\"] == pid].copy()\n",
    "\n",
    "        # preds map\n",
    "        preds_map = {}\n",
    "        for _, row in tdf.iterrows():\n",
    "            pred = str(row[\"pred\"]).strip()\n",
    "            preds_map[row[\"task_code\"]] = [] if pred in [\"\", \"nan\", \"None\"] else pred.split(\";\")\n",
    "\n",
    "        task_codes = list(tdf[\"task_code\"].values)\n",
    "        order = topological_order(task_codes, preds_map)\n",
    "\n",
    "        # CPM baseline: use likely_m only\n",
    "        cpm_dur_map = {row[\"task_code\"]: float(row[\"likely_m\"]) for _, row in tdf.iterrows()}\n",
    "        cpm_duration = critical_path_duration(order, preds_map, cpm_dur_map)\n",
    "        burn = float(pinfo[\"burn_rate_per_day\"])\n",
    "        fixed_cost = float(pinfo[\"fixed_cost\"])\n",
    "        cpm_cost = cpm_duration * burn + fixed_cost\n",
    "\n",
    "        durations = np.zeros(N_ITER, dtype=float)\n",
    "        costs = np.zeros(N_ITER, dtype=float)\n",
    "\n",
    "        # Monte Carlo\n",
    "        for i in range(N_ITER):\n",
    "            base_dur = {}\n",
    "            for _, row in tdf.iterrows():\n",
    "                o = float(row[\"opt_o\"])\n",
    "                m = float(row[\"likely_m\"])\n",
    "                p = float(row[\"pess_p\"])\n",
    "                base_dur[row[\"task_code\"]] = float(rng.triangular(o, m, p))\n",
    "\n",
    "            adj_dur, extra_cost = apply_risks_once(base_dur, rdf, rng)\n",
    "\n",
    "            proj_duration = critical_path_duration(order, preds_map, adj_dur)\n",
    "            proj_cost = proj_duration * burn + fixed_cost + extra_cost\n",
    "\n",
    "            durations[i] = proj_duration\n",
    "            costs[i] = proj_cost\n",
    "\n",
    "        # stats\n",
    "        def pct(x, q): return float(np.quantile(x, q))\n",
    "        d_mean = float(durations.mean())\n",
    "        c_mean = float(costs.mean())\n",
    "\n",
    "        d_p50, d_p80, d_p90 = pct(durations, 0.50), pct(durations, 0.80), pct(durations, 0.90)\n",
    "        c_p50, c_p80, c_p90 = pct(costs, 0.50), pct(costs, 0.80), pct(costs, 0.90)\n",
    "\n",
    "        # overrun probabilities vs CPM baseline\n",
    "        p_cost_over = float((costs > cpm_cost).mean())\n",
    "        p_sched_over = float((durations > cpm_duration).mean())\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"project_id\": pid,\n",
    "            \"size_bucket\": pinfo[\"size_bucket\"],\n",
    "            \"risk_level_bucket\": pinfo[\"risk_level_bucket\"],\n",
    "            \"late_concentration\": pinfo[\"late_concentration\"],\n",
    "            \"tail_type\": pinfo[\"tail_type\"],\n",
    "            \"coupling\": pinfo[\"coupling\"],\n",
    "            \"n_tasks\": int(pinfo[\"n_tasks\"]),\n",
    "            \"n_streams\": int(pinfo[\"n_streams\"]),\n",
    "            \"burn_rate_per_day\": burn,\n",
    "            \"fixed_cost\": fixed_cost,\n",
    "\n",
    "            \"cpm_duration\": cpm_duration,\n",
    "            \"cpm_cost\": cpm_cost,\n",
    "\n",
    "            \"mc_duration_mean\": d_mean,\n",
    "            \"mc_duration_p50\": d_p50,\n",
    "            \"mc_duration_p80\": d_p80,\n",
    "            \"mc_duration_p90\": d_p90,\n",
    "\n",
    "            \"mc_cost_mean\": c_mean,\n",
    "            \"mc_cost_p50\": c_p50,\n",
    "            \"mc_cost_p80\": c_p80,\n",
    "            \"mc_cost_p90\": c_p90,\n",
    "\n",
    "            \"p_duration_over_cpm\": p_sched_over,\n",
    "            \"p_cost_over_cpm\": p_cost_over,\n",
    "        })\n",
    "\n",
    "        # optional long format (for ML later)\n",
    "        # keep it lighter: store only percentiles per project OR store sampled points\n",
    "        # Here: store sampled points but you can turn it off if file gets big.\n",
    "        for i in range(N_ITER):\n",
    "            long_rows.append({\n",
    "                \"project_id\": pid,\n",
    "                \"duration\": durations[i],\n",
    "                \"cost\": costs[i],\n",
    "                \"cpm_duration\": cpm_duration,\n",
    "                \"cpm_cost\": cpm_cost,\n",
    "            })\n",
    "\n",
    "        print(f\"{pid}: CPM {cpm_duration:.1f}d | MC P50 {d_p50:.1f}d P90 {d_p90:.1f}d | P(cost>CPM)={p_cost_over:.2f}\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values([\"risk_level_bucket\", \"size_bucket\", \"project_id\"])\n",
    "    summary_path = os.path.join(OUT_DIR, \"project_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "    long_df = pd.DataFrame(long_rows)\n",
    "    long_path = os.path.join(OUT_DIR, \"sim_long.csv\")\n",
    "    long_df.to_csv(long_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", summary_path)\n",
    "    print(\" -\", long_path)\n",
    "    print(\"\\nSummary head:\")\n",
    "    print(summary_df.head(8))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Environment1]",
   "language": "python",
   "name": "conda-env-Environment1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
